# Attention

## Attention and Memory

![[Pasted image 20250504222653.png]]
![[Pasted image 20250504222850.png]]
- å›¾ä¸­å†…å®¹å±•ç¤ºäº†æ³¨æ„åŠ›å’Œè®°å¿†åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„ä¸‰ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šæ„Ÿè§‰è®°å¿†ï¼ˆSensory Memoryï¼‰ã€å·¥ä½œè®°å¿†ï¼ˆWorking Memoryï¼‰å’Œé•¿æœŸè®°å¿†ï¼ˆLong-term Memoryï¼‰ã€‚
- æ„Ÿè§‰è®°å¿†ï¼ˆSensory Memoryï¼‰æŒ‡çš„æ˜¯å¯¹å½“å‰æ„Ÿå®˜==è¾“å…¥çš„ç›´æ¥æ„ŸçŸ¥å’Œå¤„ç†==ã€‚åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œè¿™é€šå¸¸ä¸æ¨¡å‹çš„==è¾“å…¥å±‚==ç›¸å…³ï¼Œè´Ÿè´£æ¥æ”¶åŸå§‹æ•°æ®ï¼Œå¦‚å›¾åƒã€å£°éŸ³æˆ–æ–‡æœ¬ï¼Œå¹¶è¿›è¡Œåˆæ­¥å¤„ç†ã€‚æ„Ÿè§‰è®°å¿†æ˜¯çŸ­æš‚çš„ï¼Œä¸»è¦æ¶‰åŠå¯¹æ•°æ®çš„å³æ—¶å¤„ç†å’Œç¼–ç ã€‚
- å·¥ä½œè®°å¿†ï¼ˆWorking Memoryï¼‰æ¶‰åŠåˆ°å¯¹==ä¿¡æ¯çš„ä¸´æ—¶å­˜å‚¨å’Œæ“ä½œ==ï¼Œä»¥ä¾¿æ‰§è¡Œå¤æ‚çš„è®¤çŸ¥ä»»åŠ¡ã€‚åœ¨æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­ï¼Œè¿™ä¸æ¨¡å‹çš„==ä¸­é—´å±‚==æœ‰å…³ï¼Œè¿™äº›å±‚è´Ÿè´£æ‰§è¡Œç‰¹å¾æå–ã€å˜æ¢å’Œç»„åˆç­‰æ“ä½œã€‚å·¥ä½œè®°å¿†å¯¹äºæ¨¡å‹è¿›è¡Œå†³ç­–å’Œé—®é¢˜è§£å†³è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒå…è®¸æ¨¡å‹åœ¨å¤„ç†æ•°æ®æ—¶ä¿æŒå¯¹å…ˆå‰ä¿¡æ¯çš„è®¿é—®ã€‚
- é•¿æœŸè®°å¿†ï¼ˆLong-term Memoryï¼‰æŒ‡çš„æ˜¯å¯¹ä¿¡æ¯çš„==é•¿æœŸå­˜å‚¨==ï¼Œè¿™äº›ä¿¡æ¯å¯ä»¥æ˜¯æ˜¾å¼çš„ï¼ˆå¦‚äº‹å®å’Œäº‹ä»¶ï¼‰æˆ–éšå¼çš„ï¼ˆå¦‚æŠ€èƒ½å’Œä¹ æƒ¯ï¼‰ã€‚åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œé•¿æœŸè®°å¿†é€šå¸¸ä¸==æ¨¡å‹çš„æƒé‡å’Œå‚æ•°ç›¸å…³==ï¼Œè¿™äº›æƒé‡å’Œå‚æ•°åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸æ–­æ›´æ–°ï¼Œä»è€Œå…è®¸æ¨¡å‹å­˜å‚¨å’Œå›å¿†å­¦åˆ°çš„çŸ¥è¯†ã€‚
---
## Attention on Sensory Information

### Machine Translation with Attention
- ![[Pasted image 20250504233812.png]]
- ä¸Šè¿°å›¾ç‰‡æ˜¯å¸¦æœ‰æ³¨æ„åŠ›æœºåˆ¶çš„==æœºå™¨ç¿»è¯‘==æ¨¡å‹ï¼Œè¿‡ç¨‹å¦‚ä¸‹ï¼š
- 1.ç¼–ç å™¨ï¼ˆEncoderï¼‰å¤„ç†è¾“å…¥åºåˆ—ï¼ˆä¾‹å¦‚ï¼Œä¸­æ–‡å¥å­ï¼‰ï¼Œç”Ÿæˆä¸€ç³»åˆ—éšè—çŠ¶æ€ $h^1,h^2,h^3,h^4$ã€‚è¿™äº›éšè—çŠ¶æ€åŒ…å«äº†è¾“å…¥åºåˆ—çš„ä¿¡æ¯ã€‚
- 2.æ³¨æ„åŠ›æƒé‡ï¼ˆAttention Weightsï¼‰é€šè¿‡è®¡ç®—æ¯ä¸ªéšè—çŠ¶æ€ $h_{i}$å¯¹å½“å‰è§£ç æ­¥éª¤çš„é‡è¦æ€§ï¼Œæ¨¡å‹ç”Ÿæˆä¸€ç»„æ³¨æ„åŠ›æƒé‡ $Î±_1,Î±_2,Î±_3,Î±_4$. è¿™äº›æƒé‡é€šè¿‡ softmax å‡½æ•°è®¡ç®—å¾—å‡ºï¼Œç¡®ä¿æ‰€æœ‰æƒé‡åŠ èµ·æ¥ç­‰äº 1
- 3.è§£ç å™¨ï¼ˆDecoderï¼‰ä½¿ç”¨ä¸Šä¸‹æ–‡å‘é‡ c1 å’Œå…ˆå‰ç”Ÿæˆçš„è¾“å‡º z0 (å…¶å®å°±æ˜¯query)æ¥ç”Ÿæˆæ–°çš„éšè—çŠ¶æ€ z1,z2 å’Œæœ€ç»ˆçš„è¾“å‡ºã€‚è¿™äº›éšè—çŠ¶æ€éšåç”¨äºç”Ÿæˆç¿»è¯‘çš„ä¸‹ä¸€ä¸ªè¯ã€‚
- 4.ä¸Šä¸‹æ–‡å‘é‡ï¼ˆContext Vectorï¼‰ä½¿ç”¨æ³¨æ„åŠ›æƒé‡ï¼Œæ¨¡å‹è®¡ç®—ä¸Šä¸‹æ–‡å‘é‡ c0ï¼Œè¿™æ˜¯ä¸€ä¸ªåŠ æƒå’Œçš„éšè—çŠ¶æ€ï¼Œè¡¨ç¤ºæ•´ä¸ªè¾“å…¥åºåˆ—çš„ä¿¡æ¯ã€‚å…¬å¼ï¼š$c^0 = \sum \alpha_i h^i$
- æ€»ç»“ï¼šæ¨¡å‹å‚æ•°é€šè¿‡è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¢¯åº¦ä¸‹é™æ–¹æ³•è¿›è¡Œå­¦ä¹ ï¼Œä»¥æœ€å°åŒ–æŸå¤±å‡½æ•°ã€‚æ³¨æ„åŠ›æƒé‡ Î± é€šè¿‡è®­ç»ƒå­¦ä¹ å¾—åˆ°ï¼Œå®ƒä»¬å†³å®šäº†è§£ç å™¨åœ¨ç”Ÿæˆæ¯ä¸ªè¯æ—¶å¯¹ç¼–ç å™¨éšè—çŠ¶æ€çš„å…³æ³¨åº¦ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹å­¦ä¹ å¦‚ä½•æ ¹æ®è¾“å…¥åºåˆ—ç”Ÿæˆæ­£ç¡®çš„è¾“å‡ºåºåˆ—ã€‚æ³¨æ„åŠ›æœºåˆ¶ä½¿æ¨¡å‹èƒ½å¤Ÿå…³æ³¨è¾“å…¥åºåˆ—ä¸­æœ€ç›¸å…³çš„éƒ¨åˆ†ï¼Œä»è€Œæé«˜ç¿»è¯‘è´¨é‡ã€‚
- ä¸ªäººç†è§£æ ¸å¿ƒæ­¥éª¤ä¸ºï¼šç”¨ Query å’Œ Key ï¼ˆå°±æ˜¯hï¼‰è®¡ç®—åŒ¹é…åˆ†æ•°ï¼ˆScoreï¼‰ï¼Œæ¯”å¦‚é€šè¿‡ç‚¹ç§¯æˆ–å…¶ä»–å‡½æ•°ã€‚å°†åˆ†æ•°é€šè¿‡ Softmax å½’ä¸€åŒ–ï¼Œå¾—åˆ°æ³¨æ„åŠ›æƒé‡ ğ›¼  ã€‚ç”¨æ³¨æ„åŠ›æƒé‡å¯¹ Value åŠ æƒæ±‚å’Œï¼Œç”Ÿæˆä¸Šä¸‹æ–‡å‘é‡ï¼ˆå¦‚å›¾ä¸­çš„ ğ‘ ï¼‰
---
## Attention Application
### Dot-Product Attention
- ![[Pasted image 20250504235411.png]]
- å®šä¹‰
  ç‚¹ç§¯æ³¨æ„åŠ›æœºåˆ¶é€šè¿‡è®¡ç®—æŸ¥è¯¢ï¼ˆqueryï¼‰å‘é‡å’Œé”®ï¼ˆkeyï¼‰å‘é‡ä¹‹é—´çš„ç‚¹ç§¯ï¼ˆå†…ç§¯ï¼‰æ¥ç¡®å®šæ³¨æ„åŠ›æƒé‡ã€‚è¿™ç§æœºåˆ¶åœ¨å¤„ç†åºåˆ—æ•°æ®æ—¶ç‰¹åˆ«æœ‰ç”¨ï¼Œå› ä¸ºå®ƒå¯ä»¥æ•æ‰åºåˆ—ä¸­å…ƒç´ ä¹‹é—´çš„å…³ç³»ã€‚
- å·¥ä½œåŸç†
  æµç¨‹åŒåˆšåˆšçš„ç¿»è¯‘ï¼Œä¸åŒçš„æ˜¯è¿™é‡Œè®¡ç®—åŒ¹é…åˆ†æ•°ç”¨çš„æ˜¯ç‚¹ç§¯ã€‚
---
### Image(video) Captioning with Attention
- (1) **Image feature extraction**: Use CNN to extract local region features of the image.

- (2) **Attention weighting**: Dynamically calculate the attention weights of different image regions when generating each word.

- (3) **Context vector generation**: Weighted sum of regional features based on weights to obtain the context vector.

- (4) **Language generation**: Combining context vectors with the current state, generate descriptive words through a decoder.
åŸºæœ¬å°±æ˜¯CNN+RNNï¼ŒRNNå¸¦äº†ä¸ªæ³¨æ„åŠ›æœºåˆ¶ã€‚
---
### Comprehension of reading

- ![[Pasted image 20250505000910.png]]
- ![[Pasted image 20250505000919.png]]
- ![[Pasted image 20250505000924.png]]
- ç»“åˆDNNï¼Œæ³¨æ„åŠ›æœºåˆ¶
---
## Attention on Memory

### Neural Turing Machines
- ![[Pasted image 20250505001604.png]]
- Neural Turing Machines essentially entail a form of neural network architecture equipped with an external memory bank, further enhancing its ability to process and manipulate data.ç¥ç»å›¾çµæœºæœ¬è´¨ä¸Šéœ€è¦ä¸€ç§é…å¤‡å¤–éƒ¨å†…å­˜åº“çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†å…¶å¤„ç†å’Œä½œæ•°æ®çš„èƒ½åŠ›ã€‚

- By utilizing attention mechanisms, the neural network can navigate and interact with the external memory, allowing for read and write operations.é€šè¿‡åˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œç¥ç»ç½‘ç»œå¯ä»¥å¯¼èˆªå¹¶ä¸å¤–éƒ¨å­˜å‚¨å™¨äº¤äº’ï¼Œä»è€Œå…è®¸è¿›è¡Œè¯»å–å’Œå†™å…¥ä½œã€‚

- This architecture enables the machine to learn algorithms, manage sequential data, and access historical information, thus broadening the spectrum of tasks that can be undertaken by neural networks.è¿™ç§æ¶æ„ä½¿æœºå™¨èƒ½å¤Ÿå­¦ä¹ ç®—æ³•ã€ç®¡ç†é¡ºåºæ•°æ®å¹¶è®¿é—®å†å²ä¿¡æ¯ï¼Œä»è€Œæ‹“å®½äº†ç¥ç»ç½‘ç»œå¯ä»¥æ‰¿æ‹…çš„ä»»åŠ¡èŒƒå›´ã€‚

- The management and optimization of external memory in Neural Turing Machines may introduce complexities related to memory allocation and access efficiency.  ç¥ç»å›¾çµæœºä¸­å¤–éƒ¨å†…å­˜çš„ç®¡ç†å’Œä¼˜åŒ–å¯èƒ½ä¼šå¼•å…¥ä¸å†…å­˜åˆ†é…å’Œè®¿é—®æ•ˆç‡ç›¸å…³çš„å¤æ‚æ€§ã€‚
---
## Summary
- ![[Pasted image 20250505001752.png]]
---
# Transformer
## Quiz
- **QUESTION1**:What problems do RNN architectures without attention have for sequence-to-sequence problems?
  Answer:
  ==1.It is hard to learn the long-distance dependencies in RNNs.==
  ==2.We cannot compute the future hidden states until the past RNN hidden states have already been computed.==
  ==3.Lack of parallelizability.==
- **QUESTION2**:In the original Transformer model, how are attention weights calculated for a given token?
  Answer:
  ==As the dot product between the token's embeddings and those of all other tokens, followed by a SoftMax operation.==
- **QUESTION3**:What is the primary advantage of using the Transformer architecture over traditional RNNs for sequence tasks?
  Answer:
  ==Ability to capture long-range dependencies without the problem of vanishing gradients==
- **QUESTION4**:What is one of the main disadvantages of transformer models for language modelling?
  Answer: ==Large memory requirements==
---
## Self-Attention

- åŸç†ï¼šæ¯ä¸ªè¯é€šè¿‡ Queryã€Key å’Œ Value è®¡ç®—å¯¹å…¶ä»–è¯çš„æ³¨æ„åŠ›æƒé‡ã€‚æ³¨æ„åŠ›åˆ†æ•°é€šè¿‡ç‚¹ç§¯è®¡ç®—ï¼š$Score = \frac{Q*K^T}{\sqrt[]{d_{k}}}$ ,å…¶ä¸­ $ğ‘‘_{K}$æ˜¯ Key çš„ç»´åº¦(ç”¨äºç¼©æ”¾é˜²æ­¢è¿‡å¤§)ã€‚Softmax å½’ä¸€åŒ–åä¸ Value åŠ æƒæ±‚å’Œï¼Œå¾—åˆ°æ–°çš„è¡¨ç¤ºã€‚
- ![[Pasted image 20250505003655.png]]
- ![[Pasted image 20250505003655.png]]
- ![[Pasted image 20250505003655.png]]
- ![[Pasted image 20250505004144.png]]
- ![[Pasted image 20250505004152.png]]
- ![[Pasted image 20250505004157.png]]
- ![[Pasted image 20250505004203.png]]
- ![[Pasted image 20250505004209.png]]
- ![[Pasted image 20250505004214.png]]

---
## Multi-head Attention

å°†è¾“å…¥åˆ†æˆå¤šä¸ªå­ç©ºé—´ï¼Œåˆ†åˆ«è®¡ç®—æ³¨æ„åŠ›ï¼Œç„¶åæ‹¼æ¥ç»“æœï¼Œå¢å¼ºæ¨¡å‹æ•æ‰ä¸åŒå…³ç³»çš„èƒ½åŠ›ã€‚ï¼ˆç®€å•æè¿°å°±æ˜¯ä»æ•´ä½“æ³¨æ„ç‰‡æ®µè½¬åŒ–ä¸ºå¤šæ–¹é¢çš„è§‚å¯Ÿ)
![[Pasted image 20250505004504.png]]
- è¾“å…¥çš„ Queryï¼ˆ$Q$ï¼‰ã€Keyï¼ˆ$K$ï¼‰å’Œ Valueï¼ˆ$V$ï¼‰åˆ†åˆ«é€šè¿‡ä¸åŒçš„çº¿æ€§å˜æ¢ï¼ˆæƒé‡çŸ©é˜µ $ğ‘Š_{i}^Q$ , $W_{i}^K$,$W_{i}^V$ï¼‰æŠ•å½±åˆ° â„ ä¸ªå­ç©ºé—´ã€‚
  å…¶ä¸­ â„ æ˜¯æ³¨æ„åŠ›å¤´çš„æ•°é‡ï¼ˆé€šå¸¸ä¸º 8 æˆ– 16ï¼‰
  ==æ¯ä¸ªå¤´çš„ç»´åº¦é€šå¸¸æ˜¯$$\frac{d_{model}}{h}$$ï¼Œç¡®ä¿æ€»ç»´åº¦ä¿æŒä¸€è‡´.==
- å†å°±æ˜¯ç‹¬ç«‹è®¡ç®—æ¯ä¸ªå¤´çš„æ³¨æ„åŠ›ï¼Œ$Head_{i}=Attention(QW_{i}^Q,KW_{i}^K,VW_{i}^V)$.
- æœ€åå°†æ‰€æœ‰å¤´çš„è¾“å‡ºæ‹¼æ¥èµ·æ¥(==Concatenate==)ï¼Œå†é€šè¿‡ä¸€ä¸ªçº¿æ€§å±‚ï¼ˆæƒé‡çŸ©é˜µ$W^O$ï¼‰è¿›è¡Œå˜æ¢å¾—åˆ°æœ€ç»ˆè¾“å‡º
- ä»¥ä¸‹æ˜¯2å¤´çš„ç¤ºä¾‹ï¼ˆå¥½æ€ªï¼‰ï¼š
  ![[Pasted image 20250505005653.png]]
---

## Transformer!

å…³é”®ç»„ä»¶å¦‚ä¸‹ï¼š
- **è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSelf-Attentionï¼‰**ï¼šè®¡ç®—åºåˆ—ä¸­æ¯ä¸ªå…ƒç´ ä¸å…¶ä»–å…ƒç´ çš„å…³è”æƒé‡ï¼Œæ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚ 
  ==**Query** vectors capture the global context==
  ==**Key** vectors determine the importance of specific elements==
  ==**Value vectors** store the intermediate feature representations==
  $$MultiHead(Q, K, V) = Concat(headâ‚, ..., headâ‚•)W^O$$
  $$headáµ¢ = Attention(QWáµ¢^Q, KWáµ¢^K, VWáµ¢^V)$$
- **å¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Head Attentionï¼‰**ï¼šå°†æ³¨æ„åŠ›æœºåˆ¶æ‹†åˆ†ä¸ºå¤šä¸ªâ€œå¤´â€ï¼Œåˆ†åˆ«å­¦ä¹ ä¸åŒçš„ç‰¹å¾è¡¨ç¤ºï¼Œå¢å¼ºæ¨¡å‹è¡¨è¾¾èƒ½åŠ›ã€‚
- **ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰**ï¼šé€šè¿‡æ·»åŠ ä½ç½®ä¿¡æ¯ï¼Œå¼¥è¡¥æ¨¡å‹å¯¹è¾“å…¥åºåˆ—é¡ºåºçš„æ„ŸçŸ¥ï¼ˆå› ä¸º Transformer æœ¬èº«ä¸åŒ…å«å¾ªç¯æˆ–å·ç§¯ç»“æ„ï¼‰ã€‚==ç”¨äºè§£å†³ï¼š temporal information is missing==
- **æ®‹å·®è¿æ¥ï¼ˆResidual Connectionï¼‰**Â å’ŒÂ **å±‚å½’ä¸€åŒ–ï¼ˆLayer Normalizationï¼‰**ï¼šç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼ŒåŠ é€Ÿè®­ç»ƒã€‚
- **ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼ˆScaled Dot-Product Attentionï¼‰**$$A(Q,K,V)=softmax\left( \frac{QK^T}{\sqrt{ d_{k} }} \right)V$$
- **ç¼–ç å™¨æ¨¡å—ï¼ˆEncoder Blockï¼‰**ï¼š
  ç»„æˆï¼šå¤šå¤´æ³¨æ„åŠ›ï¼Œ2 å±‚å‰é¦ˆç¥ç»ç½‘ç»œ (NN)ï¼ˆå¸¦ ReLUï¼‰
  ç‰¹æ€§ï¼šæ®‹å·®è¿æ¥ï¼Œå±‚å½’ä¸€åŒ–ï¼ˆLayerNormï¼‰
  é€šè¿‡ LayerNorm(x + sublayer(x)) ä½¿è¾“å…¥æ¯å±‚å’Œæ¯ä¸ªè®­ç»ƒç‚¹å‡å€¼ä¸º 0ã€æ–¹å·®ä¸º 1
- å¦‚å›¾ï¼Œæ˜¯**Transformerçš„å„ä¸ªæ¨¡å—**![[Pasted image 20250505012553.png]]
- è¯¦è§£Link[ç‚¹æˆ‘](https://bgg.medium.com/seq2seq-pay-attention-to-self-attention-part-2-%E4%B8%AD%E6%96%87%E7%89%88-ef2ddf8597a4)
---
ä¼˜ç‚¹ï¼š
- **é•¿è·ç¦»ä¾èµ–å»ºæ¨¡**ï¼ŒSelf-Attention èƒ½ç›´æ¥æ•æ‰åºåˆ—ä¸­ä»»æ„ä¸¤ä¸ªå…ƒç´ çš„å…³ç³»ï¼Œé¿å…äº†LSTMå’ŒRNNæ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜
- æ”¯æŒå¹¶è¡ŒåŒ–è®¡ç®—

ç¼ºç‚¹ï¼š
- è®¡ç®—èµ„æºéœ€æ±‚å¤§
- **ç¼–ç å™¨è¾“å…¥**ç¼ºå°‘æ—¶é—´ä¿¡æ¯------->è§£å†³æ–¹æ¡ˆå°±æ˜¯å¼•å…¥ä½ç½®ç¼–ç (positional encoding)

---
**Positional Encoding**
- å°±è®°ç€å¶æ•°åˆ—ç”¨sin,å¥‡æ•°åˆ—ç”¨cos
- ![[Pasted image 20250505012430.png]]
- ![[Pasted image 20250505012502.png]]
---
**Summary**
![[Pasted image 20250505012733.png]]

---
QUESTION:
- Which of the following properties will a good position encoding ideally have
- ***Answer:***
  *1.Unique for all positions*
  *2.Relative distances are independent of absolute sequence position*
  *3.Well-defined for arbitrary sequence lengths*
- 