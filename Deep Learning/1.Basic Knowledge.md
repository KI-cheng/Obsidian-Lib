# Deep learning framework

![[Pasted image 20250423161751.png]]
深度学习框架是用于构建、训练和部署深度学习模型的工具。以下是一些流行的深度学习框架：
- TensorFlow：由 Google 开发，支持多种平台和语言，具有强大的功能和灵活的架构。
- PyTorch：由 Facebook 的 AI 研究团队开发，以动态计算图和易用性著称，广泛用于研究和开发。
- Keras：一个高级神经网络 API，可以运行在 TensorFlow、CNTK 或 Theano 之上，易于使用，适合快速原型设计。
- MXNet：一个高效的开源深度学习框架，支持灵活和高效的 GPU 计算。
- Caffe/Caffe2：由加州大学伯克利分校的贾扬清博士开发，广泛用于计算机视觉任务。
---
# Common models

深度学习模型由多层神经网络组成，每层包含多个神经元。以下是一些常见的模型类型：
- 卷积神经网络（Convolutional Neural Networks, CNNs）：适用于图像处理任务，如图像分类、目标检测和图像分割。
- 循环神经网络（Recurrent Neural Networks, RNNs）：适用于序列数据，如时间序列预测、自然语言处理和语音识别。
- 长短期记忆网络（Long Short-Term Memory, LSTM）：一种特殊的 RNN，能够处理长序列数据中的长期依赖问题。
- 生成对抗网络（Generative Adversarial Networks, GANs）：由生成器和判别器组成，用于生成新的数据样本，如图像、文本和音频。
- Transformer：基于自注意力机制的模型，广泛用于自然语言处理任务，如机器翻译、文本生成和问答系统。
---
# Stacked Generalization
堆叠泛化，简称Stacking或Blending。
堆叠泛化是一种高级的集成学习方法，通过构建层次化的模型结构，利用初级模型的输出作为次级模型的输入，以达到超越单个模型性能的效果。
**基本原理**
堆叠泛化的核心在于构建一个多层的模型结构，主要包含两个层次：
- 1.初级层（Base Layer）：该层包含多个独立训练的基学习器（如决策树、神经网络、支持向量机等）。这些学习器使用原始特征数据进行训练，并各自产生对测试集样本的预测输出。
- 2.次级层（Meta Layer）：这一层包含一个或多个元学习器（Meta-Learner），它们接收初级层各基学习器对同一样本的预测结果作为新特征，并以此为基础进行训练，旨在学习如何最佳地结合初级模型的输出以做出最终预测。元学习器可以是线性回归、逻辑回归、神经网络等任何通用的学习算法。

**实现步骤**
- 1.**数据划分**：将原始数据集划分为训练集、验证集和测试集。训练集用于训练初级模型，验证集用于训练元学习器，而测试集用于评估最终堆叠模型的整体性能。
- 2.**初级模型训练**：在训练集上分别训练多个基学习器，确保它们之间具有多样性。记录每个模型在验证集上的预测输出。
- 3.**次级模型训练**：将初级模型在验证集上的预测输出作为新的特征向量，用这些特征向量及对应的验证集真实标签来训练元学习器。元学习器的目标是学习如何最优地结合初级模型的预测以接近真实标签。
- 4.**堆叠模型构建**：使用训练好的初级模型对测试集进行预测，生成新的特征向量。然后，使用训练好的元学习器对这些特征向量进行最终预测，得到堆叠模型的输出。

**优缺点**
**优点**：
- 1.通过结合多个模型的预测结果，可以提高预测精度和鲁棒性。
- 2.强调初级模型之间的差异性，有助于捕获数据的不同侧面和复杂性。
**缺点**：
- 1.模型的训练和预测过程相对复杂，需要较多的计算资源和时间。
- 2.在实际应用中，如果堆叠的层次过多或模型选择不当，可能会导致过拟合。
---
# Deep v.s. Shallow model
## Representation Learning & Deep Learning 
![[Pasted image 20250423162758.png]]

---
## Definition
### Deep model

- 1.深度学习是一种机器学习的方法，通过建立多层神经网络结构，模拟人脑神经元的连接方式，实现对输入数据的分类、识别等任务。
- 2.在语音识别中，深度学习算法可以自动学习语音的特征，提高语音识别的准确率。
![[Pasted image 20250423163012.png]]

---
### Shallow model

1.浅层学习通常指的是传统的机器学习算法，如支持向量机（SVM）、朴素贝叶斯（Naive Bayes）等。
2.这些算法主要依赖于人工设计的特征提取方法，如梅尔频率倒谱系数（MFCC）、线性预测系数（LPC）等。
GMM、DCT、DFT在深度学习中分别扮演着不同的角色。GMM用于建模混合分布和优化深度学习模型；DCT用于图像压缩和特征提取；DFT虽然使用相对较少，但仍然是信号处理领域的重要工具；而对数函数则用于数据的缩放、归一化和损失函数的定义等。

![[Pasted image 20250423162957.png]]

---
## Comparison of ML&DL

![[Pasted image 20250423163258.png]]
![[Pasted image 20250423163304.png]]

# Deep Neural Network
## Single Neuron

![[Pasted image 20250423164235.png]]

**定义与结构**
深度学习中的单个神经元模拟了生物神经元的基本工作原理。它接收多个输入信号，对这些信号进行加权求和，并应用一个非线性激活函数来产生输出。这个输出可以作为其他神经元的输入，或者作为神经网络的最终输出。
具体来说，单个神经元的结构包括：
- 输入：来自其他神经元或外部数据源的信号。
- 权重：每个输入信号都有一个与之对应的权重，表示该信号对神经元输出的影响程度。
- 偏置：一个常数项，用于调整神经元的输出阈值。
- 激活函数：一个非线性函数，用于将加权求和后的结果映射到输出范围。

**工作原理**
   1.输入信号加权求和：神经元接收多个输入信号，每个信号都乘以一个权重，然后将这些加权后的信号求和。
   2.应用偏置：将求和结果加上一个偏置值，得到神经元的净输入。
   3.激活函数处理：将净输入传递给激活函数，得到神经元的输出。

**激活函数**
激活函数是神经元中的关键组成部分，它引入了非线性，使得神经网络能够处理复杂的非线性问题。常见的激活函数包括：
- Sigmoid函数：将输入映射到0和1之间，常用于二分类问题的输出层。
- Tanh函数：将输入映射到-1和1之间，具有更好的对称性，常用于隐藏层。
- ReLU函数：线性整流函数，当输入大于0时输出等于输入，否则输出为0，具有简单、高效的特点。

**神经元在深度学习中的作用**
   1.特征提取：通过组合不同的权重和偏置，神经元可以从输入数据中提取出有用的特征。
   2.非线性变换：激活函数引入了非线性，使得神经网络能够逼近复杂的非线性函数。
   3.信息传递：神经元的输出可以作为其他神经元的输入，从而实现信息的传递和整合。

---
## Neural network
### Structure
![[Pasted image 20250423164311.png]]

---
### Depth and width

更深的网络意味着更详细的特征提取和更多的参数。一般选用更深的网络而不是更宽的网络.

**Fat + Shallow（宽而浅）**
- 1.定义：这种网络结构具有较宽的层（即每层包含较多的神经元）和较少的层数（即网络深度较浅）。
- 2.特点：由于每层神经元数量多，因此参数总量可能较大，能够捕捉更丰富的特征。然而，较浅的网络深度可能**限制了其学习复杂特征**的能力。==虽然每层计算量大，但是计算步骤少，每层计算可以并行化，时间复杂度**低==**

** Thin + Deep（窄而深）**
- 1.定义：这种网络结构具有较窄的层（即每层包含较少的神经元）和较多的层数（即网络深度较深）。
- 2.特点：通过增加网络深度，可以**学习更抽象、更复杂的特征**表示。虽然每层神经元数量少，但整体参数数量可能通过多层叠加而保持或超过宽而浅的网络。此外，较深的网络通常具有**更好的梯度流动和训练动态**。==用于层数更多，顺序计算的特性导致时间复杂度高==
---
## How to apply

在构建学习问题时，我们需要清晰地定义输入域（Input domain）和输出域（Output domain），以及它们之间的映射关系，这个映射关系通常由学习算法f来实现。

####  1.Input domain
	输入域X是算法f接收的原始数据。根据你的需求，输入域可以是多种形式的数据：
	•单词：用于文本分类、情感分析等任务。
	•词序列：用于自然语言处理任务，如机器翻译、词性标注等。
	•音频信号：用于语音识别、音乐分类等任务。
	•点击日志：用于推荐系统、用户行为分析等任务。

#### 2.Output domain
	输出域Y是算法f需要预测或生成的目标。它同样可以是多种形式：
	•单个标签：用于分类任务，如将邮件分类为垃圾邮件或正常邮件。
	•序列标签：用于序列标注任务，如词性标注，其中每个单词都被赋予一个词性标签。
	•树结构：用于语法分析任务，如构建句子的句法树。
	•概率分布：用于回归任务或生成模型，如预测某个事件发生的概率。

#### 3.Learning Algorithm f
	即定义映射关系
	映射关系f: X → Y是学习算法的核心，它将输入域X中的数据映射到输出域Y中的目标。
	这通常涉及到机器学习的某个或多个模型，如神经网络、决策树、支持向量机等。

---
## Overall process

**1.定义问题：**
   确定目标： 明确你想要解决的问题是什么，比如分类、回归、聚类、生成等。

**2.数据收集：** 
   收集与问题相关的数据。对于深度学习，这通常意味着大量的标记或未标记数据。
   数据预处理：
- 清洗： 去除噪声和异常值。
- 标准化/归一化： 使数据具有相同的尺度，例如，将像素值缩放到[0,1]或[-1,1]。
- 增强： 通过旋转、缩放、裁剪等方法增加数据多样性。
- 分割： 将数据分为训练集、验证集和测试集。

**3.特征工程：**
   - 手工特征： 对于传统机器学习，这一步很重要。但在深度学习中，模型会自动学习特征。
   - 自动特征学习： 利用深度学习模型自动从原始数据中提取特征。

**4.模型设计：**
   - 选择架构： 根据问题选择合适的网络架构，如CNN、RNN、Transformer等。
   - 确定层数和神经元数量： 设计网络的深度和宽度。
   - 激活函数： 选择合适的激活函数，如ReLU、Sigmoid、Tanh等。
   - 损失函数： 根据问题类型选择合适的损失函数，如交叉熵损失、均方误差损失等。

**5.模型训练：**
   - 优化算法： 选择合适的优化器，如SGD、Adam、RMSprop等。
   - 学习率调整： 设定初始学习率，并可能使用学习率衰减策略。
   - 权重初始化： 选择合适的权重初始化方法，如Xavier初始化、He初始化等。
   - 正则化： 应用dropout、L1/L2正则化等技术防止过拟合。

**5.批量处理：** 
   确定批量大小，进行小批量梯度下降。

**6.模型评估与调优：**
   - 性能评估： 使用验证集评估模型性能。
   - 超参数调优： 调整学习率、批量大小、网络架构等超参数。
   - 早停法： 通过监控验证集上的性能来防止过拟合。

**7.模型部署：**
   - 模型保存： 保存训练好的模型。
   - 模型推理： 在新数据上使用模型进行预测。

**8.模型解释与可视化：**
   - 特征可视化： 可视化网络学到的特征，了解模型是如何理解数据的。
   - 注意力机制： 对于某些模型，如Transformer，可以分析注意力权重来理解模型的决策过程。
---
## Key Components

**1.数据（The Data）**
- 作用： 数据是机器学习模型学习的基础。它包括输入特征和对应的标签或结果。
- 类型： 数据可以是监督学习（有标签的数据）、无监督学习（无标签的数据）、半监督学习或强化学习（通过与环境的交互获得反馈）。
- 质量： 数据的质量直接影响模型的性能。数据需要是准确、干净、有代表性的。
- 预处理： 数据通常需要经过预处理步骤，如清洗、标准化、归一化、编码类别特征等。

**2.模型（The Model）**
- 作用： 模型是数据转换的数学表示，它定义了输入数据和输出结果之间的关系。
- 类型： 模型可以是线性回归、决策树、神经网络等。在深度学习中，模型通常指的是具有多个层的神经网络，如CNN、RNN、Transformer等。
- 参数： 模型包含需要学习的参数，这些参数决定了模型如何从输入映射到输出。

**3.目标函数（The Objective Function）**
- 作用： 目标函数（也称为损失函数或代价函数）衡量模型预测与实际结果之间的差异，指导模型训练的方向。
- 类型： 常见的目标函数包括均方误差（MSE）用于回归问题，交叉熵损失用于分类问题等。
- 优化： 目标函数的设计需要反映问题的本质，以便模型能够学习到正确的模式。

**如何优化目标函数？**
- 直接优化（Direct Optimization）:
   - 有些目标函数（如平方误差）是光滑且易于优化的，因为它们是连续可微的，可以使用梯度下降等方法直接优化。
- 替代目标（Surrogate Objective）:
	- 有些目标函数（如错误率）直接优化比较困难，因为它们可能不是连续可微的（例如，分类问题中的0-1损失），或者优化起来计算成本很高。
	- 在这种情况下，我们通常会优化一个替代目标函数，这个函数更容易计算和优化，但与原始目标函数有相似的性质。例如，在分类问题中，我们可能使用交叉熵损失作为替代目标，因为它是可微的，并且当类别分布接近真实分布时，交叉熵损失会接近0-1损失。

**训练集上的损失最小化**
- 我们通过在训练集上最小化损失函数来学习模型参数的最佳值。这个过程涉及到调整模型的参数，以减少预测值和实际标签之间的差异。
**泛化能力（Generalization）**
- 泛化的定义：泛化能力指的是模型在未见过的数据上的表现能力。一个模型如果能够泛化，那么它在新数据上的表现应该接近在训练数据上的表现。
	- 过拟合（Overfitting）：如果模型在训练数据上表现很好，但在未见过的数据上表现差，这种情况称为过拟合。过拟合通常是因为模型过于复杂，捕捉到了训练数据中的噪声和偶然模式。
	- 欠拟合（Underfitting）：如果模型在训练数据上表现就不好，那么可能是模型太简单，没有捕捉到数据的基本结构和模式。

**4.学习算法（The Learning Algorithm）**
- 作用： 学习算法是调整模型参数以最小化目标函数的过程。
- 类型： 包括梯度下降、随机梯度下降（SGD）、Adam、RMSprop等。
- 优化策略： 学习算法可能包括动量、学习率衰减、早停等策略来提高训练效率和模型性能。
- 反向传播： 在深度学习中，学习算法通常依赖于反向传播算法来计算梯度，并更新模型参数。
---
